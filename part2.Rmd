---
title: "Part 2"
author: "Your Name"
date: "05/02/2019"
---

## Set up
We're going to need a package called `rvest` (rvest, harvest, geddit?) to do our web scraping, `robotstxt` helps with some of our ethics checking, and `stringr`and `dplyr` will help with our data cleaning.

```{r set-up, message=FALSE, warning=FALSE}
list.of.packages = c("dplyr", "ggplot2", "lubridate", "purrr", "readr", "rvest", "stringr", "robotstxt", "ratelimitr", "tidyr")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

# Main scraping package
library(rvest)

# Helpful for ethical behaviour
library(robotstxt)
library(ratelimitr)

# Tidying data and other useful things
library(dplyr)
library(ggplot2)
library(lubridate)
library(purrr)
library(readr)
library(stringr)
library(tidyr)
library(httr)

```

NOTE! We would usually do a lot more preparation talking about the ethics of web scraping, as well as how websites are built (HTML, CSS and URLs). Unfortunately, we don't have time for that, but those are all elements you might learn more about doing data science.

In your browser, you can got to [https://www.fanfiction.net/robots.txt](www.fanfiction.net/robots.txt), or check what it says directly from your R session!

```{r ethics, warning=FALSE}

rt = robotstxt("fanfiction.net")

# Check which areas are off limits
rt$permissions

# Any mention of a crawl delay?
rt$crawl_delay

# Is it enough to just check this?

n = 1 # n is the number of allowed events within a period
p = 10 # p is the length (in seconds) of measurement period

```

```{r scrape-data}

# Pick which page you want to scrape. For this to work with my code, use the "Browse" option, not search.

# This url will get us the the top favourited Harry Potter stories of all ratings. "https://www.fanfiction.net/book/Harry-Potter/?&srt=4&r=10&p="

base_url = 
  "https://www.fanfiction.net/book/Harry-Potter/?&srt=4&r=10&p="

# 1:4 indicates the pages I want
# There are 25 stories per page, so the first 4 pages gives us the top 100 stories
map_df(1:4, limit_rate(function(i){

  pages = read_html(paste0(base_url, i))
  
  # The '.' before stitle and xgray indicates that it is the class
  data.frame(title = html_text(html_nodes(pages, ".stitle")), details=html_text(html_nodes(pages, ".xgray")),
             stringsAsFactors=FALSE)

}, rate(n, p)) ) -> my_data

# This may take a little while to run - just sit tight

# Store how many stories for each graph titles later
n_stories = dim(my_data)[1]

# Did you get the number of stories you expected?
n_stories

```

## Get some helpful information from the filter menu
This next section pulls some information from the filters menu to help up clean up the data later. Optional: see [this xpath tutorial](https://www.w3schools.com/xml/xpath_intro.asp) for more information.

```{r helpful-bits}
# Major credit to Anna for this code and the chunk below - you would have been disgusted by how hack-y it was before

helpful = str_split_fixed(base_url, "\\?", 2)[1]

home = read_html(helpful)

# Get genres
genres = home %>% 
  html_nodes(xpath='//*[@id="myform"]/div[1]/select[3]/option') %>%
  html_text() 
genres = genres[-1] #delete first text value
genresReg = paste(genres, collapse = '|')

# Get the list of characters
characters <- home %>% 
  html_nodes(xpath='//*[@id="myform"]/div[1]/select[10]/option') %>%
  html_text() 
charactersReg = paste(characters, collapse = '|')

# Get ratings
ratingLevels <- c("K", "K+", "T", "M")

# Useful for later
my_counter = function(x){
 return(length(x) - sum(str_detect(x, "Not")))
}

```

## Tidy up the data
```{r tidy-data}

tidy_data = my_data %>%
  mutate(id = 1:n()) %>%
  select(id,title, details) %>% 
  separate_rows(details,sep="- ") %>%
  separate(details,c("varName", "tempValue"), sep=":", fill="left") %>%
  mutate(
    newValue = str_trim(tempValue),
    newVarName = case_when(
      !is.na(varName) ~ varName,
      str_detect(newValue, "Complete") ~ "Completed",
      str_detect(newValue, genresReg) ~ "Genres",
      str_detect(newValue, charactersReg) ~ "Characters",
      TRUE ~ "Language"
    )
  ) %>% 
  select(id, title, newVarName, newValue) %>%
  spread(newVarName, newValue, fill = NA) %>%
  mutate_at(vars(c("Chapters","Favs","Follows","Reviews","Words")), parse_number) %>%
  mutate_at(vars(c("Published")), mdy) %>%
  mutate(
    Rating = factor(Rated, ratingLevels),
    Completed = case_when(is.na(Completed) ~ "Not complete", 
                          TRUE ~ "Complete"),
    Characters = str_replace_all(Characters,c("\\[" = "","\\]" = ""))
  )

# Make columns for each genre

for (ii in 1:length(genres)){
    tidy_data = tidy_data %>%
    mutate_(.dots = setNames(list(paste0("case_when(str_detect(Genres, \"", genres[ii], "\") ~ \"", genres[ii], "\",
                                        TRUE ~ paste0(\"Not \", \"", genres[ii], "\"),
                                        NA ~ paste0(\"Not \",\"", genres[ii], "\"))"
                                        )), paste0("genre_", genres[ii])))
}

# Make columns for each character
# This one takes a while because there are a lot of characters that could be included

for (jj in 1:length(characters)){
    tidy_data = tidy_data %>%
    mutate_(.dots = setNames(list(paste0("case_when(str_detect(Characters, \"", characters[jj], "\") ~ \"", characters[jj], "\",
                                        TRUE ~ paste0(\"Not \", \"", characters[jj], "\"),
                                        NA ~ paste0(\"Not \",\"", characters[jj], "\"))"
                                        )), paste0("character_", characters[jj])))
}

# This part makes a nice bonus dataset by character, instead of by story

character_data = tidy_data %>% 
  gather(character_Character, Character, starts_with("character_")) %>% 
  filter(!grepl("Not ",Character)) %>% 
  select(id, Character, everything(), -character_Character) %>% 
  group_by(Character) %>% 
  mutate(char_count = n()) 

```



```{r save-data}

# This will save your data as two csv files for us to use in Part 3
write_csv(tidy_data, "my_story_data.csv")
write_csv(character_data, "my_character_data.csv")

```

Return to the computer lab instructions now!
